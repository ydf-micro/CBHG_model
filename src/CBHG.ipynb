{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "['lv4', 'shi4', 'yang2', 'chun1', 'yan1', 'jing3', 'da4', 'kuai4', 'wen2', 'zhang1', 'de', 'di3', 'se4', 'si4', 'yue4', 'de', 'lin2', 'luan2', 'geng4', 'shi4', 'lv4', 'de2', 'xian1', 'huo2', 'xiu4', 'mei4', 'shi1', 'yi4', 'ang4', 'ran2']\n['绿', '是', '阳', '春', '烟', '景', '大', '块', '文', '章', '的', '底', '色', '四', '月', '的', '林', '峦', '更', '是', '绿', '得', '鲜', '活', '秀', '媚', '诗', '意', '盎', '然']\n\n['ta1', 'jin3', 'ping2', 'yao1', 'bu4', 'de', 'li4', 'liang4', 'zai4', 'yong3', 'dao4', 'shang4', 'xia4', 'fan1', 'teng2', 'yong3', 'dong4', 'she2', 'xing2', 'zhuang4', 'ru2', 'hai3', 'tun2', 'yi1', 'zhi2', 'yi3', 'yi1', 'tou2', 'de', 'you1', 'shi4', 'ling3', 'xian1']\n['他', '仅', '凭', '腰', '部', '的', '力', '量', '在', '泳', '道', '上', '下', '翻', '腾', '蛹', '动', '蛇', '行', '状', '如', '海', '豚', '一', '直', '以', '一', '头', '的', '优', '势', '领', '先']\n\n['pao4', 'yan3', 'da3', 'hao3', 'le', 'zha4', 'yao4', 'zen3', 'me', 'zhuang1', 'yue4', 'zheng4', 'cai2', 'yao3', 'le', 'yao3', 'ya2', 'shu1', 'de', 'tuo1', 'qu4', 'yi1', 'fu2', 'guang1', 'bang3', 'zi', 'chong1', 'jin4', 'le', 'shui3', 'cuan4', 'dong4']\n['炮', '眼', '打', '好', '了', '炸', '药', '怎', '么', '装', '岳', '正', '才', '咬', '了', '咬', '牙', '倏', '地', '脱', '去', '衣', '服', '光', '膀', '子', '冲', '进', '了', '水', '窜', '洞']\n\n['ke3', 'shei2', 'zhi1', 'wen2', 'wan2', 'hou4', 'ta1', 'yi1', 'zhao4', 'jing4', 'zi', 'zhi3', 'jian4', 'zuo3', 'xia4', 'yan3', 'jian3', 'de', 'xian4', 'you4', 'cu1', 'you4', 'hei1', 'yu3', 'you4', 'ce4', 'ming2', 'xian3', 'bu4', 'dui4', 'cheng1']\n['可', '谁', '知', '纹', '完', '后', '她', '一', '照', '镜', '子', '只', '见', '左', '下', '眼', '睑', '的', '线', '又', '粗', '又', '黑', '与', '右', '侧', '明', '显', '不', '对', '称']\n\n['qi1', 'shi2', 'nian2', 'dai4', 'mo4', 'wo3', 'wai4', 'chu1', 'qiu2', 'xue2', 'mu3', 'qin1', 'ding1', 'ning2', 'wo3', 'chi1', 'fan4', 'yao4', 'xi4', 'jue2', 'man4', 'yan4', 'xue2', 'xi2', 'yao4', 'shen1', 'zuan1', 'xi4', 'yan2']\n['七', '十', '年', '代', '末', '我', '外', '出', '求', '学', '母', '亲', '叮', '咛', '我', '吃', '饭', '要', '细', '嚼', '慢', '咽', '学', '习', '要', '深', '钻', '细', '研']\n\n['yi1', 'jin4', 'men2', 'wo3', 'bei4', 'jing1', 'dai1', 'le', 'zhe4', 'hu4', 'ming2', 'jiao4', 'pang2', 'ji2', 'de', 'lao3', 'nong2', 'shi4', 'kang4', 'mei3', 'yuan2', 'chao2', 'fu4', 'shang1', 'hui2', 'xiang1', 'de', 'lao3', 'bing1', 'qi1', 'zi', 'zhang3', 'nian2', 'you3', 'bing4', 'jia1', 'tu2', 'si4', 'bi4', 'yi1', 'pin2', 'ru2', 'xi3']\n['一', '进', '门', '我', '被', '惊', '呆', '了', '这', '户', '名', '叫', '庞', '吉', '的', '老', '农', '是', '抗', '美', '援', '朝', '负', '伤', '回', '乡', '的', '老', '兵', '妻', '子', '长', '年', '有', '病', '家', '徒', '四', '壁', '一', '贫', '如', '洗']\n\n['zou3', 'chu1', 'cun1', 'zi', 'lao3', 'yuan3', 'lao3', 'yuan3', 'wo3', 'hai2', 'hui2', 'tou2', 'zhang1', 'wang4', 'na4', 'ge4', 'an1', 'ning2', 'tian2', 'jing4', 'de', 'xiao3', 'yuan4', 'na4', 'ge4', 'shi3', 'wo3', 'zhong1', 'shen1', 'nan2', 'wang4', 'de', 'xiao3', 'yuan4']\n['走', '出', '村', '子', '老', '远', '老', '远', '我', '还', '回', '头', '张', '望', '那', '个', '安', '宁', '恬', '静', '的', '小', '院', '那', '个', '使', '我', '终', '身', '难', '忘', '的', '小', '院']\n\n['er4', 'yue4', 'si4', 'ri4', 'zhu4', 'jin4', 'xin1', 'xi1', 'men2', 'wai4', 'luo2', 'jia1', 'nian3', 'wang2', 'jia1', 'gang1', 'zhu1', 'zi4', 'qing1', 'wen2', 'xun4', 'te4', 'de', 'cong2', 'dong1', 'men2', 'wai4', 'gan3', 'lai2', 'qing4', 'he4']\n['二', '月', '四', '日', '住', '进', '新', '西', '门', '外', '罗', '家', '碾', '王', '家', '冈', '朱', '自', '清', '闻', '讯', '特', '地', '从', '东', '门', '外', '赶', '来', '庆', '贺']\n\n['dan1', 'wei4', 'bu4', 'shi4', 'wo3', 'lao3', 'die1', 'kai1', 'de', 'ping2', 'shen2', 'me', 'yao4', 'yi1', 'ci4', 'er4', 'ci4', 'zhao4', 'gu4', 'wo3', 'wo3', 'bu4', 'neng2', 'ba3', 'zi4', 'ji3', 'de', 'bao1', 'fu2', 'wang3', 'xue2', 'xiao4', 'shuai3']\n['单', '位', '不', '是', '我', '老', '爹', '开', '的', '凭', '什', '么', '要', '一', '次', '二', '次', '照', '顾', '我', '我', '不', '能', '把', '自', '己', '的', '包', '袱', '往', '学', '校', '甩']\n\n['dou1', 'yong4', 'cao3', 'mao4', 'huo4', 'ge1', 'bo2', 'zhou3', 'hu4', 'zhe', 'wan3', 'lie4', 'lie4', 'ju1', 'ju1', 'chuan1', 'guo4', 'lan4', 'ni2', 'tang2', 'ban1', 'de', 'yuan4', 'ba4', 'pao3', 'hui2', 'zi4', 'ji3', 'de', 'su4', 'she3', 'qu4', 'le']\n['都', '用', '草', '帽', '或', '胳', '膊', '肘', '护', '着', '碗', '趔', '趔', '趄', '趄', '穿', '过', '烂', '泥', '塘', '般', '的', '院', '坝', '跑', '回', '自', '己', '的', '宿', '舍', '去', '了']\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "with open('../data/zh.tsv', 'r', encoding='utf-8') as f:\n",
    "    data = f.readlines()\n",
    "\n",
    "    inputs = []\n",
    "    labels = []\n",
    "    for i in range(len(data)):\n",
    "        key, pny, word = data[i].split('\\t')\n",
    "        inputs.append(pny.split(' '))\n",
    "        labels.append(word.strip('\\n').split(' '))\n",
    "    \n",
    "    for i in range(10):\n",
    "        print(inputs[i])\n",
    "        print(labels[i])\n",
    "        print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "拼音词表长度为: 1152\n文字词表长度为: 4460\n['<PAD>', 'lv4', 'shi4', 'yang2', 'chun1', 'yan1', 'jing3', 'da4', 'kuai4', 'wen2']\n['<PAD>', '绿', '是', '阳', '春', '烟', '景', '大', '块', '文']\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "def get_vocab(data):\n",
    "    vocab = ['<PAD>']\n",
    "    for line in data:\n",
    "        for char in line:\n",
    "            if char not in vocab:\n",
    "                vocab.append(char)\n",
    "                \n",
    "    return vocab\n",
    "\n",
    "pny2id = get_vocab(inputs)\n",
    "word2id = get_vocab(labels)\n",
    "\n",
    "print(f'拼音词表长度为: {len(pny2id)}')\n",
    "print(f'文字词表长度为: {len(word2id)}')\n",
    "\n",
    "print(pny2id[:10])\n",
    "print(word2id[:10])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "input_num = [[pny2id.index(pny) for pny in line] for line in inputs]\n",
    "label_num = [[word2id.index(word) for word in line] for line in labels]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "[[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  11  16  17\n   18   2   1  19  20  21  22  23  24  25  26  27   0   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 28  29  30  31  32  11  33  34  35  36  37  38  39  40  41  36  42  43\n   44  45  46  47  48  49  50  51  49  52  11  53   2  54  20   0   0   0\n    0   0   0   0   0   0   0]\n [ 55  56  57  58  59  60  61  62  63  64  15  65  66  67  59  67  68  69\n   11  70  71  49  72  73  74  75  76  77  59  78  79  42   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 80  81  82   9  83  84  28  49  85  86  75  87  88  89  39  56  90  11\n   91  92  93  92  94  95  92  96  97  98  32  99 100   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [101 102 103 104 105 106 107 108 109 110 111 112 113 114 106 115 116  61\n  117 118 119 120 110 121  61 122 123 117 124   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 49  77 125 106 126 127 128  59 129 130  97 131 132 133  11 134 135   2\n  136 137 138 139 140 141 142 143  11 134 144 101  75 145 103 146 147 148\n  149  14 150  49 151  46 152]\n [153 108 154  75 134 155 134 155 106 156 142  52  10 157 158 159 160 114\n  161  86  11 162 163 158 159 164 106 165 122 166 157  11 162 163   0   0\n    0   0   0   0   0   0   0]\n [167  15  14 168 169  77 170 171 125 107 172 148 173 174 148 175 176 177\n  178   9 179 180  11 181 182 125 107 183 184 185 186   0   0   0   0   0\n    0   0   0   0   0   0   0]]\n[[  1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  11  16  17\n   18   2   1  19  20  21  22  23  24  25  26  27   0   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 28  29  30  31  32  11  33  34  35  36  37  38  39  40  41  42  43  44\n   45  46  47  48  49  50  51  52  50  53  11  54  55  56  57   0   0   0\n    0   0   0   0   0   0   0]\n [ 58  59  60  61  62  63  64  65  66  67  68  69  70  71  62  71  72  73\n   74  75  76  77  78  79  80  81  82  83  62  84  85  86   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 87  88  89  90  91  92  93  50  94  95  81  96  97  98  39  59  99  11\n  100 101 102 101 103 104 105 106 107 108 109 110 111   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [112 113 114 115 116 117 118 119 120 121 122 123 124 125 117 126 127 128\n  129 130 131 132 121 133 128 134 135 129 136   0   0   0   0   0   0   0\n    0   0   0   0   0   0   0]\n [ 50  83 137 117 138 139 140  62 141 142 143 144 145 146  11 147 148   2\n  149 150 151 152 153 154 155 156  11 147 157 158  81 159 114 160 161 162\n  163  14 164  50 165  47 166]\n [167 119 168  81 147 169 147 169 117 170 155  53 171 172 173 174 175 176\n  177 178  11 179 180 173 174 181 117 182 183 184 185  11 179 180   0   0\n    0   0   0   0   0   0   0]\n [186  15  14 187 188  83 189 190 137 118 191 162 192 193 162 194 195 196\n  197 198 199 200  74 201 202 137 118 203 204 205 206   0   0   0   0   0\n    0   0   0   0   0   0   0]]\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_batch(input_data, label_data, batch_size):\n",
    "    batch_num = len(input_data) // batch_size\n",
    "    for k in range(batch_num):\n",
    "        begin = k * batch_size\n",
    "        end = begin + batch_size\n",
    "        input_batch = input_data[begin: end]\n",
    "        label_batch = label_data[begin: end]\n",
    "        max_len = max([len(line) for line in input_batch])\n",
    "        input_batch = np.array([line + [0] * (max_len-len(line)) for line in input_batch])\n",
    "        label_batch = np.array([line + [0] * (max_len-len(line)) for line in label_batch])\n",
    "        yield input_batch, label_batch\n",
    "        \n",
    "batch = get_batch(input_num, label_num, 8)\n",
    "input_batch, label_batch = next(batch)\n",
    "\n",
    "print(input_batch)\n",
    "print(label_batch)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# embedding\n",
    "\n",
    "def embedding(inputs,\n",
    "              vocab_size,\n",
    "              num_units,\n",
    "              zero_pad=True,\n",
    "              scope='embedding',\n",
    "              reuse=None):\n",
    "    \n",
    "    '''\n",
    "    Embeds a given tensor.\n",
    "    \n",
    "    :param inputs: A 'Tensor' with type 'int32' or 'int64' containing\n",
    "                    the ids to be looked up in 'lookup table'.\n",
    "    :param vocab_size: int. Vocabulary size\n",
    "    :param num_units: int. Number of embedding hidden units\n",
    "    :param zero_pad: boolean. If true, all the values of the first row\n",
    "                    (id 0) should be constant zeros.\n",
    "    :param scale: boolean, If true. the outputs is multiplied by sqrt\n",
    "                    num_units.\n",
    "    :param scope: Optional scope for 'variable_scope'\n",
    "    :param reuse: Boolean, whether to reuse the weights of a previous\n",
    "                layer by the same name.\n",
    "    :return: \n",
    "        A Tensor with one more rank than inputs`s. The last dimensionality\n",
    "        should be 'num_units'\n",
    "        \n",
    "        For example,\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=True)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[ 0.          0.        ]\n",
    "      [ 0.09754146  0.67385566]\n",
    "      [ 0.37864095 -0.35689294]]\n",
    "     [[-1.01329422 -1.09939694]\n",
    "      [ 0.7521342   0.38203377]\n",
    "      [-0.04973143 -0.06210355]]]\n",
    "    ```\n",
    "\n",
    "    ```\n",
    "    import tensorflow as tf\n",
    "\n",
    "    inputs = tf.to_int32(tf.reshape(tf.range(2*3), (2, 3)))\n",
    "    outputs = embedding(inputs, 6, 2, zero_pad=False)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        print sess.run(outputs)\n",
    "    >>\n",
    "    [[[-0.19172323 -0.39159766]\n",
    "      [-0.43212751 -0.66207761]\n",
    "      [ 1.03452027 -0.26704335]]\n",
    "     [[-0.11634696 -0.35983452]\n",
    "      [ 0.50208133  0.53509563]\n",
    "      [ 1.22204471 -0.96587461]]]\n",
    "    ```\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        lookup_table = tf.get_variable('lookup_table',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab_size, num_units],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "        if zero_pad:\n",
    "            lookup_table = tf.concat((tf.zeros(shape=[1, num_units]),\n",
    "                                      lookup_table[1:, :]), 0)\n",
    "        outputs = tf.nn.embedding_lookup(lookup_table, inputs)\n",
    "        \n",
    "        \n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "\n",
    "# pre-net\n",
    "\n",
    "def prenet(inputs,\n",
    "           num_units=None, \n",
    "           is_training=True,\n",
    "           scope='prenet',\n",
    "           reuse=None,\n",
    "           dropout_rate=0.2):\n",
    "    '''\n",
    "    Pre-net for Encoder and Decoder\n",
    "    \n",
    "    :param inputs: A 2D or 3D tensor.\n",
    "    :param num_units: A list of two integers. or None.\n",
    "    :param is_training: A python boolean.\n",
    "    :param scope: Optional scope for 'variable_scope'\n",
    "    :param reuse: Boolean, whether to reuse the weights of a previous layer by\n",
    "                    the same name.\n",
    "    :param dropout_rate: \n",
    "    :return: \n",
    "        A 3D tensor of shape [N, T, num_units/2].\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        outputs = tf.layers.dense(inputs, units=num_units[0], \n",
    "                                  activation=tf.nn.relu, name='dense1')\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate, \n",
    "                                    training=is_training, name='dropout1')\n",
    "        outputs = tf.layers.dense(outputs, units=num_units[1],\n",
    "                                  activation=tf.nn.relu, name='dense2')\n",
    "        outputs = tf.layers.dropout(outputs, rate=dropout_rate,\n",
    "                                    training=is_training, name='dropout2')\n",
    "        \n",
    "    return outputs      # (N, ..., num_units[1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "\n",
    "# conv1d \n",
    "\n",
    "def conv1d(inputs,\n",
    "           filters=None,\n",
    "           kernel_size=1,\n",
    "           rate=1,\n",
    "           padding='SAME',\n",
    "           use_bias=False,\n",
    "           activation_fn=None,\n",
    "           scope='conv1d',\n",
    "           reuse=None):\n",
    "    '''\n",
    "    \n",
    "    :param inputs: A 3D tensor with shape of [batch, time, depth]\n",
    "    :param filters: An int, Number of outputs (activation maps)\n",
    "    :param size: An int. Filter size.\n",
    "    :param rate: Dilation rate.\n",
    "    :param padding: Either 'same' or 'valid' or 'causal' (case-insensitive).\n",
    "    :param use_bias: A boolean.\n",
    "    :param activation_fn: \n",
    "    :param scope: Optional scope for 'variable_scope'\n",
    "    :param reuse: Boolean. whether to reuse the weights of a previous\n",
    "                    layer by the same name.\n",
    "    :return: \n",
    "        A masked tensor of the same shape and dtypes as 'inputs'\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope):\n",
    "        if padding.lower() == 'causal':\n",
    "            # pre-padding for causality\n",
    "            pad_len = (kernel_size - 1) * rate  # padding size\n",
    "            inputs = tf.pad(inputs, [[0, 0], [pad_len, 0], [0, 0]])\n",
    "            padding = 'valid'\n",
    "            \n",
    "        if filters is None:\n",
    "            filters = inputs.get_shape().as_list[-1]\n",
    "            \n",
    "        params = {'inputs': inputs, 'filters': filters, \n",
    "                  'kernel_size': kernel_size,\n",
    "                  'dilation_rate': rate, 'padding': padding, \n",
    "                  'activation': activation_fn, 'use_bias': use_bias,\n",
    "                  'reuse': reuse}\n",
    "        \n",
    "        outputs = tf.layers.conv1d(**params)\n",
    "        \n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "\n",
    "def normalize(inputs,\n",
    "              decay=0.99,\n",
    "              epsilon=1e-8,\n",
    "              is_training=True,\n",
    "              activation_fn=None,\n",
    "              reuse=None,\n",
    "              scope='normalize'):\n",
    "    '''\n",
    "        Applies batch/layer normalization.\n",
    "        \n",
    "    :param inputs: A tensor with 2 or more dimensions, \n",
    "        where the first dimension has\n",
    "        `batch_size`. If type is `bn`, the normalization is over all but\n",
    "        the last dimension. Or if type is `ln`, the normalization is over\n",
    "        the last dimension. Note that this is different from the native\n",
    "        `tf.contrib.layers.batch_norm`. For this I recommend you change\n",
    "        a line in ``tensorflow/contrib/layers/python/layers/layer.py`\n",
    "        as follows.\n",
    "        Before: mean, variance = nn.moments(inputs, axis, keep_dims=True)\n",
    "        After: mean, variance = nn.moments(inputs, [-1], keep_dims=True)\n",
    "    :param decay: Decay for the moving average. Reasonable values for `decay` are close\n",
    "        to 1.0, typically in the multiple-nines range: 0.999, 0.99, 0.9, etc.\n",
    "        Lower `decay` value (recommend trying `decay`=0.9) if model experiences\n",
    "        reasonably good training performance but poor validation and/or test\n",
    "        performance.\n",
    "    :param epsilon: \n",
    "    :param is_training: Whether or not the layer is in training mode. W\n",
    "    :param activation_fn: Activation function.\n",
    "    :param reuse: \n",
    "    :param scope: Optional scope for `variable_scope`.\n",
    "\n",
    "    :return: \n",
    "            A tensor with the same shape and data dtype as `inputs`.\n",
    "    '''\n",
    "    \n",
    "    inputs_shape = inputs.get_shape()\n",
    "    inputs_rank = inputs_shape.ndims\n",
    "    \n",
    "    # use fused batch norm if inputs_rank in [2, 3, 4] as it is much faster.\n",
    "    # pay attention to the fact that fused_batch_norm requires shape to \n",
    "    # be rank 4 of NHWC\n",
    "    \n",
    "    inputs = tf.expand_dims(inputs, axis=1)\n",
    "    outputs = tf.contrib.layers.batch_norm(inputs=inputs,\n",
    "                                           decay=decay,\n",
    "                                           center=True,\n",
    "                                           scale=True,\n",
    "                                           updates_collections=None,\n",
    "                                           is_training=is_training,\n",
    "                                           scope=scope,\n",
    "                                           zero_debias_moving_mean=True,\n",
    "                                           fused=True,\n",
    "                                           reuse=reuse)\n",
    "    outputs = tf.squeeze(outputs, axis=1)\n",
    "    \n",
    "    if activation_fn:\n",
    "        outputs = activation_fn(outputs)\n",
    "        \n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "\n",
    "def conv1d_bank(inputs,\n",
    "                num_units=None,\n",
    "                K =16,\n",
    "                is_training=True,\n",
    "                scope='conv1d_banks',\n",
    "                reuse=None):\n",
    "    '''\n",
    "    Applies a series of conv1d separately\n",
    "    \n",
    "    N: batch size\n",
    "    T: time steps\n",
    "    C: embedding hidden units\n",
    "    \n",
    "    :param inputs: A 3d tensor with shape of [N, T, C]\n",
    "    :param num_units: \n",
    "    :param K: An int. The size of conv1d banks. That is,\n",
    "            The 'inputs' are convolved with K filters: 1, 2, ..., K.\n",
    "    :param is_training: A boolean. This is passed to an argument \n",
    "                        of 'batch_normalize'\n",
    "    :param scope: \n",
    "    :param reuse: \n",
    "    :return: \n",
    "        A 3d tensor with shape of [N, T, K*Hp.embed_size//2]\n",
    "    '''\n",
    "    \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        outputs = conv1d(inputs, num_units // 2, 1)\n",
    "        for k in range(2, K+1):\n",
    "            with tf.variable_scope(f'num_{k}'):\n",
    "                output = conv1d(inputs, num_units, k)\n",
    "                outputs = tf.concat((outputs, output), -1)\n",
    "                \n",
    "        outputs = normalize(outputs, is_training=is_training,\n",
    "                            activation_fn=tf.nn.relu)\n",
    "        \n",
    "    return outputs  # (N, T, Hp.embed_size//2*K)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "\n",
    "def lstm(inputs, \n",
    "         num_units=None, \n",
    "         bidirection=False, \n",
    "         seqlen=None, \n",
    "         scope='lstm',\n",
    "         reuse=None):\n",
    "    '''\n",
    "    Applies a lstm\n",
    "    \n",
    "    :param inputs: A 3d tensor with shape of [N, T, C].\n",
    "    :param num_units: An int. The number of hidden units.\n",
    "    :param bidirection: A boolean. If True, bidirectional results\n",
    "                        are concatenated.\n",
    "    :param seqlen: \n",
    "    :param scope: Optional scope for `variable_scope`.\n",
    "    :param reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "                by the same name.\n",
    "    :return: \n",
    "        If bidirection is True, a 3d tensor with shape \n",
    "        of [N, T, 2*num_units], otherwise [N, T, num_units].\n",
    "    '''\n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        if num_units is None:\n",
    "            num_units = inputs.get_shape().as_list[-1]\n",
    "            \n",
    "        cell = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "        if bidirection:\n",
    "            cell_bw = tf.nn.rnn_cell.LSTMCell(num_units)\n",
    "            outputs, _ = tf.nn.bidirectional_dynamic_rnn(cell, cell_bw,\n",
    "                                                         inputs, \n",
    "                                                         sequence_length=seqlen,\n",
    "                                                         dtype=tf.float32)\n",
    "            return tf.concat(outputs, 2)\n",
    "        else:\n",
    "            outputs, _ = tf.nn.dynamic_rnn(cell, inputs,\n",
    "                                           sequence_length=seqlen,\n",
    "                                           dtype=tf.float32)\n",
    "            \n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def highwaynet(inputs, \n",
    "               num_units=None, \n",
    "               scope='highwaynet', \n",
    "               reuse=None):\n",
    "    '''\n",
    "    Highway networks\n",
    "    \n",
    "    :param inputs: A 3D tensor of shape [N, T, W].\n",
    "    :param num_units: An int or `None`. Specifies the number of units in the highway layer\n",
    "                    or uses the input size if `None`.\n",
    "    :param scope: Optional scope for `variable_scope`.\n",
    "    :param reuse: Boolean, whether to reuse the weights of a previous layer\n",
    "        by the same name.\n",
    "    :return: \n",
    "            A 3D tensor of shape [N, T, W].\n",
    "    '''\n",
    "    \n",
    "    if not num_units:\n",
    "        num_units = inputs.get_shape()[-1]\n",
    "        \n",
    "    with tf.variable_scope(scope, reuse=reuse):\n",
    "        H = tf.layers.dense(inputs, units=num_units, \n",
    "                            activation=tf.nn.relu, name='dense1')\n",
    "        T = tf.layers.dense(inputs, units=num_units,\n",
    "                            activation=tf.nn.sigmoid,\n",
    "                            bias_initializer=tf.constant_initializer(-1.0),\n",
    "                            name='dense2')\n",
    "        \n",
    "        C = 1. - T\n",
    "        \n",
    "        outputs = H * T + inputs * C\n",
    "        \n",
    "    return outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "\n",
    "class CBHG():\n",
    "    '''Builds a model graph'''\n",
    "    \n",
    "    def __init__(self, arg):\n",
    "        tf.reset_default_graph()\n",
    "        self.pny_size = arg.pny_size\n",
    "        self.word_size = arg.word_size\n",
    "        self.embed_size = arg.embed_size\n",
    "        self.is_training = arg.is_training\n",
    "        self.num_highwaynet_blocks = arg.num_highwaynet_blocks\n",
    "        self.encoder_num_banks = arg.encoder_num_banks\n",
    "        self.lr = arg.lr\n",
    "        \n",
    "        self.x = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        self.y = tf.placeholder(tf.int32, shape=(None, None))\n",
    "        \n",
    "        # character Embedding for x\n",
    "        enc = embedding(self.x, self.pny_size, \n",
    "                        self.embed_size, scope='emb_x')\n",
    "        \n",
    "        # Encoder pre-net\n",
    "        prenet_out = prenet(enc,\n",
    "                            num_units=[self.embed_size, self.embed_size//2],\n",
    "                            is_training=self.is_training) # (N, T, E/2)\n",
    "        \n",
    "        # Encoder CBHG\n",
    "        # Conv1D bank\n",
    "        enc = conv1d_bank(prenet_out, \n",
    "                          K=self.encoder_num_banks,\n",
    "                          num_units=self.embed_size//2,\n",
    "                          is_training=self.is_training) # (N, T, K*E/2)\n",
    "        \n",
    "        # Max pooling\n",
    "        enc = tf.layers.max_pooling1d(enc, 2, 1, padding='same')\n",
    "        \n",
    "        # Conv1D projections\n",
    "        enc = conv1d(enc, self.embed_size//2, 5, scope='conv1d_1') # (N, T, E/2)\n",
    "        enc = normalize(enc, is_training=self.is_training,\n",
    "                        activation_fn=tf.nn.relu, scope='norm1')\n",
    "        enc = conv1d(enc, self.embed_size//2, 5, scope='conv1d_2') # (N, T, E/2)\n",
    "        enc = normalize(enc, is_training=self.is_training,\n",
    "                        activation_fn=None, scope='norm2')\n",
    "        enc += prenet_out  # (N, T, E/2) # residual connections\n",
    "        \n",
    "        # Highway Nets\n",
    "        for i in range(self.num_highwaynet_blocks):\n",
    "            enc = highwaynet(enc, num_units=self.embed_size//2,\n",
    "                             scope=f'highwaynet_{i}') # (N, T, E/2)\n",
    "            \n",
    "        # bidirectional lstm\n",
    "        enc = lstm(enc, self.embed_size//2, True, scope='lstm1')\n",
    "        \n",
    "        # Readout\n",
    "        self.outputs = tf.layers.dense(enc, self.word_size, use_bias=False)\n",
    "        self.preds = tf.to_int32(tf.argmax(self.outputs, axis=-1))\n",
    "        \n",
    "        if self.is_training:\n",
    "            self.loss = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y,\n",
    "                                                                       logits=self.outputs)\n",
    "            self.istarget = tf.to_float(tf.not_equal(self.y, tf.zeros_like(self.y))) # masking\n",
    "            self.hits = tf.to_float(tf.equal(self.preds, self.y)) * self.istarget\n",
    "            self.acc = tf.reduce_sum(self.hits) / tf.reduce_sum(self.istarget)\n",
    "            self.mean_loss = tf.reduce_sum(self.loss * self.istarget) / tf.reduce_sum(self.istarget)\n",
    "            \n",
    "            # Training Scheme\n",
    "            self.global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "            self.optimizer = tf.train.AdamOptimizer(learning_rate=self.lr)\n",
    "            self.train_op = self.optimizer.minimize(self.mean_loss, global_step=self.global_step)\n",
    "            \n",
    "            # Summary\n",
    "            tf.summary.scalar('mean_loss', self.mean_loss)\n",
    "            tf.summary.scalar('acc', self.acc)\n",
    "            self.merged = tf.summary.merge_all()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "\n",
    "def create_hparams():\n",
    "    params = tf.contrib.training.HParams(\n",
    "        # vocab\n",
    "        pny_size = 50,\n",
    "        word_size = 50,\n",
    "        # embedding size\n",
    "        embed_size = 300,\n",
    "        num_highwaynet_blocks = 4,\n",
    "        encoder_num_banks = 8,\n",
    "        lr = 0.001,\n",
    "        is_training = True)\n",
    "    \n",
    "    return params\n",
    "\n",
    "arg = create_hparams()\n",
    "arg.pny_size = len(pny2id)\n",
    "arg.word_size = len(word2id)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "epochs 1 :average loss =  1.0292338697305523\n",
      "epochs 2 :average loss =  0.4339743798907101\n",
      "epochs 3 :average loss =  0.32146026409301703\n",
      "epochs 4 :average loss =  0.2568893333812958\n",
      "epochs 5 :average loss =  0.2165330571583802\n",
      "epochs 6 :average loss =  0.18923001680379892\n",
      "epochs 7 :average loss =  0.16972323978510528\n",
      "epochs 8 :average loss =  0.15588265168689705\n",
      "epochs 9 :average loss =  0.14461949134341714\n",
      "epochs 10 :average loss =  0.1374188847327826\n",
      "epochs 11 :average loss =  0.13156710992185136\n",
      "epochs 12 :average loss =  0.12802302987271877\n",
      "epochs 13 :average loss =  0.12582620804804512\n",
      "epochs 14 :average loss =  0.1233449401968287\n",
      "epochs 15 :average loss =  0.12516787370735658\n",
      "epochs 16 :average loss =  0.12656381167512273\n",
      "epochs 17 :average loss =  0.13058180372398157\n",
      "epochs 18 :average loss =  0.13388134208514327\n",
      "epochs 19 :average loss =  0.14362116484162576\n",
      "epochs 20 :average loss =  0.15529048121328284\n",
      "epochs 21 :average loss =  0.1701302073367387\n",
      "epochs 22 :average loss =  0.19050324588094172\n",
      "epochs 23 :average loss =  0.21913168235238117\n",
      "epochs 24 :average loss =  0.23819724992568075\n",
      "epochs 25 :average loss =  0.2626191081862644\n",
      "训练时间:75.24\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "epochs = 25\n",
    "batch_size = 16\n",
    "\n",
    "g = CBHG(arg)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    merged = tf.summary.merge_all()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    if os.path.exists('logs/model.meta'):\n",
    "        saver.restore(sess, '/logs/model')\n",
    "        \n",
    "    writer = tf.summary.FileWriter('tensorboard/lm', tf.get_default_graph())\n",
    "    for k in range(epochs):\n",
    "        total_loss =  0\n",
    "        batch_num = len(input_num) // batch_size\n",
    "        batch = get_batch(input_num, label_num, batch_size)\n",
    "        for i in range(batch_num):\n",
    "            input_batch, label_batch = next(batch)\n",
    "            feed = {g.x: input_batch, g.y: label_batch}\n",
    "            cost, _ = sess.run([g.mean_loss, g.train_op], feed_dict=feed)\n",
    "            total_loss += cost\n",
    "            if (k * batch_num + i) % 10 == 0:\n",
    "                rs = sess.run(merged, feed_dict=feed)\n",
    "                writer.add_summary(rs, k * batch_num + i)\n",
    "                \n",
    "        print('epochs', k+1, ':average loss = ', total_loss/batch_num)\n",
    "            \n",
    "    saver.save(sess, '../logs/model')\n",
    "    writer.close()\n",
    "\n",
    "end = time.time()\n",
    "print(f'训练时间:{(end-start)/60:.2f}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "INFO:tensorflow:Restoring parameters from ../logs/model\n",
      "赛距为职业运动员最长二十四千米业馀运动员最长二十一千米青年运动员最长十五千米\n",
      "绿是阳春烟景大快文章的底色四月的林峦更是绿得鲜活秀媚诗艺盎然\n",
      "今天天气不错\n",
      "天气很热\n",
      "模行推断\n",
      "输入测试拼音\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "\n",
    "arg.is_training = False\n",
    "\n",
    "g = CBHG(arg)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "with tf.Session() as sess:\n",
    "    saver.restore(sess, '../logs/model')\n",
    "    while True:\n",
    "        line = input('输入测试拼音: ')\n",
    "        if line == 'exit': break\n",
    "        line = line.strip('\\n').split(' ')\n",
    "        x = np.array([pny2id.index(pny) for pny in line])\n",
    "        x = x.reshape(1, -1)\n",
    "        preds = sess.run(g.preds, {g.x: x})\n",
    "        result = ''.join(word2id[idx] for idx in preds[0])\n",
    "        print(result)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}